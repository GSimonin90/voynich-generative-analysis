{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoxOrugrBbPS",
        "outputId": "85404d91-c8f6-4467-b3e9-6ea0a0cc557e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text has 27 unique characters\n",
            "\n",
            "--- Mapping Example ---\n",
            "fachys ykal ar  ----> [ 7  2  4  9 25 20  1 25 12  2 13  1  2 19  1]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Load the cleaned text\n",
        "path_to_file = 'voynich_super_clean.txt'\n",
        "text = open(path_to_file, 'r', encoding='utf-8').read()\n",
        "\n",
        "# Create the vocabulary: the set of all unique characters in the text\n",
        "vocab = sorted(set(text))\n",
        "print(f'The text has {len(vocab)} unique characters')\n",
        "\n",
        "# Create maps to convert characters to numbers and vice versa\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# Vectorize the text: transform the entire text into a sequence of integers\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "print('\\n--- Mapping Example ---')\n",
        "print(f'{text[:15]} ----> {text_as_int[:15]}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the length of character sequences\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create the training dataset\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "# Transform the dataset into sequences of 101 characters\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# Function to create input -> target pairs\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Apply the function to all sequences\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Shuffle the dataset and create batches\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print('\\n--- Dataset Structure ---')\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRMNetT9CTuk",
        "outputId": "3c3120ad-8b32-48a7-f57a-4f607521d9bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Dataset Structure ---\n",
            "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary (number of unique characters)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Dimension of the embedding (how \"rich\" the vector for each character is)\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of neurons in the LSTM layer\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        # 1. Embedding Layer: transforms numbers into vectors\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "\n",
        "        # 2. LSTM Layer: the core that learns the sequences\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                             return_sequences=True,\n",
        "                             stateful=True,\n",
        "                             recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "        # 3. Output Layer: produces probabilities for the next character\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "# Show a summary of the model's architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "vPJzllUhCXx2",
        "outputId": "3bd6e3d6-7d70-4c6e-c786-c2bda365c2a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and the optimizer\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Configuration to save model \"checkpoints\" during training\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "# Set the number of epochs (how many times the model reads the entire text)\n",
        "EPOCHS = 20\n",
        "\n",
        "print(\"\\n--- 🚀 Starting Training ---\")\n",
        "# This process can take 30-60 minutes or more\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "print(\"--- ✅ Training Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhfEQsqOCb68",
        "outputId": "5c496ee7-5903-44b3-9092-2c740eab8d9a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 🚀 Starting Training ---\n",
            "Epoch 1/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 69ms/step - loss: 2.0545\n",
            "Epoch 2/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 70ms/step - loss: 1.3236\n",
            "Epoch 3/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 71ms/step - loss: 1.2804\n",
            "Epoch 4/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 72ms/step - loss: 1.2500\n",
            "Epoch 5/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 72ms/step - loss: 1.2356\n",
            "Epoch 6/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 74ms/step - loss: 1.2167\n",
            "Epoch 7/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 1.1910\n",
            "Epoch 8/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 75ms/step - loss: 1.1661\n",
            "Epoch 9/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 73ms/step - loss: 1.1326\n",
            "Epoch 10/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 73ms/step - loss: 1.0835\n",
            "Epoch 11/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 72ms/step - loss: 1.0320\n",
            "Epoch 12/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 72ms/step - loss: 0.9709\n",
            "Epoch 13/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 72ms/step - loss: 0.8975\n",
            "Epoch 14/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 73ms/step - loss: 0.8184\n",
            "Epoch 15/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - loss: 0.7393\n",
            "Epoch 16/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - loss: 0.6676\n",
            "Epoch 17/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 73ms/step - loss: 0.6035\n",
            "Epoch 18/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - loss: 0.5497\n",
            "Epoch 19/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 73ms/step - loss: 0.5037\n",
            "Epoch 20/20\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 73ms/step - loss: 0.4632\n",
            "--- ✅ Training Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild the model with batch_size=1\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "# \"Build\" the model BEFORE loading the weights\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "# Specify the exact name of the last checkpoint\n",
        "last_checkpoint_file = os.path.join(checkpoint_dir, f\"ckpt_{EPOCHS}.weights.h5\")\n",
        "\n",
        "# Now that the model is built, we can load the weights\n",
        "model.load_weights(last_checkpoint_file)\n",
        "\n",
        "print(f\"✅ Weights loaded from '{last_checkpoint_file}'. Model is ready for generation.\")"
      ],
      "metadata": {
        "id": "uUTVLnIuCfvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f538b5-c010-4075-ab45-959ee316ea1b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Weights loaded from './training_checkpoints/ckpt_20.weights.h5'. Model is ready for generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Cell: Generation and Saving of Texts\n",
        "\n",
        "def generate_text(model, start_string, num_generate=50000, temp=1.0):\n",
        "    \"\"\"Generates text using the trained model.\"\"\"\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "    temperature = temp\n",
        "\n",
        "    # The 'model.reset_states()' line is no longer needed in recent TF versions\n",
        "\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))\n",
        "\n",
        "# --- EXECUTION BLOCK FOR THE NEW WORK ---\n",
        "\n",
        "print(\"--- 🤖 Starting Generation with the 'Super Clean' Model ---\")\n",
        "\n",
        "start_seed = \"daiin \"\n",
        "\n",
        "# --- 1. Normal Temperature (0.7) ---\n",
        "print(\"Generating text at normal temperature (0.7)...\")\n",
        "generated_text_normal = generate_text(model, start_string=start_seed, temp=0.7)\n",
        "\n",
        "with open(\"generated_clean_normal_temp.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(generated_text_normal)\n",
        "print(\"✅ Text at temp 0.7 saved to 'generated_clean_normal_temp.txt'.\")\n",
        "\n",
        "# --- 2. Low Temperature (0.5) ---\n",
        "print(\"\\nGenerating text at low temperature (0.5)...\")\n",
        "generated_text_low = generate_text(model, start_string=start_seed, temp=0.5)\n",
        "\n",
        "with open(\"generated_clean_low_temp.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(generated_text_low)\n",
        "print(\"✅ Text at temp 0.5 saved to 'generated_clean_low_temp.txt'.\")\n",
        "\n",
        "# --- 3. High Temperature (1.2) ---\n",
        "print(\"\\nGenerating text at high temperature (1.2)...\")\n",
        "generated_text_high = generate_text(model, start_string=start_seed, temp=1.2)\n",
        "\n",
        "with open(\"generated_clean_high_temp.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(generated_text_high)\n",
        "print(\"✅ Text at temp 1.2 saved to 'generated_clean_high_temp.txt'.\")\n",
        "\n",
        "print(\"\\n--- All generations are complete. You can now download the files and analyze them. ---\")"
      ],
      "metadata": {
        "id": "dFKhMcUACims",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e0f785e-de32-47b5-a1e8-73ec696553da"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 🤖 Starting Generation with the 'Super Clean' Model ---\n",
            "Generating text at normal temperature (0.7)...\n",
            "✅ Text at temp 0.7 saved to 'generated_clean_normal_temp.txt'.\n",
            "\n",
            "Generating text at low temperature (0.5)...\n",
            "✅ Text at temp 0.5 saved to 'generated_clean_low_temp.txt'.\n",
            "\n",
            "Generating text at high temperature (1.2)...\n",
            "✅ Text at temp 1.2 saved to 'generated_clean_high_temp.txt'.\n",
            "\n",
            "--- All generations are complete. You can now download the files and analyze them. ---\n"
          ]
        }
      ]
    }
  ]
}